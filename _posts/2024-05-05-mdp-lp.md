---
title: 'Solving Markov Decision Process with linear programming tutorial'
date: 2024-05-05
permalink: /posts/2024/05/mdp-lp-en/
tags:
  - Markov decision process
  - linear programming
  - MDP
  - lp
---

# Quick Introduction

A Markov decision process (MDP) is a mathematical model that is often used for the stochastic decision-making process of a dynamic system. In this context, the system's outcomes are influenced by both random factors and the decisions made by an agent, who must make a series of sequential decisions over time. The basic building block of an MDP consists of 4 tuples $(S, A, P_a, R_a)$, where:
* $S$ is a set of states, which is often called *state space*.
* $A$ is a set of actions called *action space* ($A_s$ would be the set of available actions from state $s$).
* $P_a(s,s') = P_a(s_{t+1} = s'\|s_t=s, a_t=a)$ is the probability of the action $a$ in state $s$ and time $t$ will lead to state $s'$.
* $R_a(s,s')$ is the immediate (or **expected immediate**) reward of taking action $a$ to transition from state $s$ to state $s'$.

<p align="center">
  <img width="400" src='/images/mdp_tut/simple_mdp.png' class="center">
</p>
<p align="center">
  <em>Figure 1. A simple example of an MDP with three states (green circles), two actions (orange circles) and two rewards (orange arrows). </em>
</p>
<br/> 

As shown in Figure 1 [[source](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/800px-Markov_Decision_Process.svg.png)], the simple MDP has three states ($S_0, S_1, S_2$). In which, for each state, there are two possible actions ($a_0,a_1$) and each action that is taken in state $s$ has its own unique set of transition probabilities that lead to the state at the next timestep $s'$. In Figure 1, the rewards are only available at the location indicated by the orange arrows, thus we can assume that the others have 0 rewards. Usually, this MDP can be assumed to run in a finite amount of timestep, often called finite horizon MDP (e.g. marketing campaign that runs for 3 weeks), or it can also be assumed to run indeterminately, often called infinite horizon MDP (e.g. a machine that runs 24/7 for 10 years).

The goal of the MDP is to find the **optimum policy** that governs the decision-maker, or agent to make the decisions that lead to the biggest cumulative reward over the given time window. For example, the optimal policy of Figure 1 could be "Always do $a_0$ in state $S_0$, always do $a_0$ in state $S_1$, and always do $a_a$ in state $S_2$" (disclaimer: I don't know the optimal solution, it's just a mere example). 

There are several methods that are commonly used to find the optimum policy such as *value iteration*, *policy iteration*, *linear programming*, etc. However, in this tutorial, we will only cover solving the MDP using **linear programming**, and I also will not cover much about the theoretical background. If you are not very familiar with the concept I strongly recommend these courses to cover the theoretical background:
* [Markov Decision Process by David Silver](https://www.youtube.com/watch?v=lfHX2hHRMVQ)
* [Basic linear programming concept](https://www.youtube.com/watch?v=Bzzqx1F23a8)

To make everything more concrete, let's consider a use case of MDP to determine the optimal policy for an industrial machine maintenance.

# Problem Statement

An old pasta machine runs 7 days a week from 10.00 to 18.00. Due to its central role in the pasta factory, they are subject to careful maintenance. However, due to its age, it doesn't have any sensors. Therefore, each morning the technicians need to inspect the machine and take note of the state of the machine which are defined as: A (best), B (moderate), C (worst). Based on the machine state reading, the production manager makes a decision based on one of the following actions:
* *Normal production*: The mechanics leave the machine in its current state, declaring it ready for operation in today's shift.
* *Basic maintenance*: The mechanics perform a quick basic maintenance in the morning which costs 3000 Eur. Conducting basic maintenance when the machine at state B can improve the machine's condition to state A with $0.2$ chance (otherwise stays in state B), and conducting basic maintenance at state C can improve the machine's condition to state B with $0.1$ chance (otherwise stays in state C). It is important to note that the machine can still **operate normally after basic maintenance** in the morning.
* *Overhaul*: Overhauling the machine will always bring the machine's condition back to state A. However, an overhaul takes the entire day and the machine simply becomes unproductive in that day. An overhaul costs 4000 Eur.

When a machine is used in production, there's a possibility of its condition deteriorating or experiencing a breakdown. The probability of this occurrence is influenced by the machine's current state. A breakdown leads to serious damage, requiring a three-day repair period (including the day of the breakdown), during which the machine cannot be used for production. On the fourth day, the machine is ready for use again in state A. On average, such a breakdown results in a net loss of 23000 Eur, factoring in all labour and material costs and the value of production before the breakdown. However, if a machine completes the entire production day without a breakdown it generates pasta that is equal to 10000 Eur of profit.

Based on the historical data, the pasta factory's chief engineer has provided us with statistical information related to the likelihood of deteriorating and breaking down. To summarize, all informations are given in the table below:

To be continued...
