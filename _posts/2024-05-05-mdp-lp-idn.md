---
title: 'Tutorial penyelesaian Markov Decision Process dengan pemrograman linier ğŸ‡®ğŸ‡©'
date: 2024-05-04
permalink: /posts/2024/05/mdp-lp-id/
tags:
  - Markov decision process
  - linear programming
  - MDP
  - lp
---
Pada postingan kali ini, kita membahas implementasi dari Markov decision process (MDP) sebagai alat untuk menyelesaikan permasalahan proses pengambilan keputusan dari sistem dinamik dengan memanfaatkan metode pemrograman linier. Pertama, kita akan membahas definisi dari MDP secara singkat. Lalu, kita akan melihat sebuah contoh kasus dari MDP untuk menentukan kebijakan optimal (*optimal policy*) dari perawatan mesin industri. Berdasarkan contoh kasus ini, kita akan membahas cara untuk menformulasikan representasi MDP yang sesuai. Terakhir, kita akan menyelesaikan representasi MDP tersebut dengan menggunakan teknik pemrograman linier untuk memperoleh kebijakan optimal dari proses pengambilan keputusan tersebut.

# Pengenalan Singkat

Markov decision process (MDP) adalah sebuah model matematis yang sering digunakan untuk memodelkan proses pengambilan keputusan stokastik dari sebuah proses dinamik. Dalam konteks ini, hasil dari sistem dipengaruhi oleh faktor random dan keputusan yang dibuat oleh agen, yang perlu membuat serangkaian keputusan berurutan dari waktu ke waktu. Komponen dasari dari MDP terdiri dari 4 *tuples* $(S, A, P_a, R_a)$, dimana:
* $S$ adalah set dari keadaan(*state*), yang seringkali disebut dengan *state space*.
* $A$ adalah set aksi yang disebut dengan *action space* ($A_s$ adalah set dari aksi yang tersedia dari keadaan $s$).
* $P_a(s,s') = P_a(s_{t+1} = s'\|s_t=s, a_t=a)$ adalah peluang dari aksi $a$ dalam keadaan $s$ dan waktu $t$ untuk bertransisi ke keadaan $s'$.
* $R_a(s,s')$ adalah nilai imbalan (*reward*) dari pengambilan aksi $a$ untuk bertransisi dari keadaan $s$ ke keadaan $s'$.

<p align="center">
  <img width="400" src='/images/mdp_tut/simple_mdp.png' class="center">
</p>
<p align="center">
  <em>Gambar 1. Contoh sederhana dari MDP dengan tiga keadaan (lingkaran hijau), dua aksi (lingkaran oranye) dan dua *rewards* (panah oranye). </em>
</p>
<br/> 

Seperti yang ditunjukkan dalam Gambar 1 [[sumber](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/800px-Markov_Decision_Process.svg.png)], MDP sederhana tersebut memiliki tiga keadaan ($S_0, S_1, S_2$). Yang dimana, untuk tiap keadaan memiliki dua kemungkinan aksi ($a_0,a_1$) dan untuk tiap aksi yang diambil pada keadaan $s$ memiliki memiliki serangkaian probabilitas transisi unik yang mengarah ke keadaan $s'$ pada langkah waktu berikutnya. Dalam Gambar 1, nilai dari *rewards* hanya terdapat pada lokasi yang diindikasikan oleh panah oranye, sehingga kita dapat berasumsi bahwa proses lainnya memiliki nilai *reward* 0. Biasanya, representasi MDP seperti ini dapat diasumsikan untuk berjalan pada jumlah langkah waktu yang terbatas, seringkali disebut dengan MDP cakrawala terbatas (*finite horizon MDP*) (contoh: kampanye pemasaran yang berjalan selama 3 minggu). Atau, MDP tersebut juga dapat diasumsikan untuk berjalan tanpa batas waktu, seringkali disebut dengan MDP cakrawala tidak terbatas (*infinite horizon MDP*) (contoh: mesin yang berjalan 24/7 selama 10 tahun.)

Tujuan dari MDP adalah menemukan **kebijakan optimal** (*optimal policy*) yang mengatur pengambil keputusan, atau agen untuk membuat keputusan yang menghasilkan imbalan kumulatif terbesar dalam jangka waktu tertentu. Misalnya, kebijakan optimal pada Gambar 1 dapat berupa "Selalu lakukan $a_0$ dalam keadaan $S_0$, selalu lakukan $a_0$ dalam keadaan $S_1$, dan selalu lakukan $a_a$ dalam keadaan $S_2$" (disklaimer: Saya tidak tahu solusi optimal sesungguhnya, itu hanya contoh belaka).

Terdapat beberapa metode yang umum digunakan untuk memperoleh kebijakan optimum, seperti *value iteration*, *policy iteration*, *dynamic programming*, dll. Namun dalam tutorial ini kita akan membahas penyelesaian MDP menggunakan **pemrograman linier**, dan saya tidak akan membahas dasar teori terlalu banyak. Jika anda kurang familiar dengan konsepnya, saya merekomendasikan dua materi yang membahas dasar teorinya:
* [Markov Decision Process by David Silver](https://www.youtube.com/watch?v=lfHX2hHRMVQ)
* [Basic linear programming concept](https://www.youtube.com/watch?v=Bzzqx1F23a8)

Agar lebih konkret, kita akan melihat contoh permasalahan MDP untuk menentukan kebijakan optimal dari proses perawatan mesin industri.

# Rumusan Masalah

Sebuah mesin pasta tua beroperasi 7 hari selama seminggu dari jam 10.00 sampai jam 18.00. Karena mesin ini sangat penting bagi pabrik pasta, mesin tersebut perlu untuk dirawat secara rutin. Namun, karena mesin tersebut sudah cukup tua, mesin tersebut tidak memiliki sensor. Sehingga setiap paginya, teknisi pabrik perlu untuk memeriksa kondisi dari mesin dan mencatatnya. Menurut standar operasi pabrik, kondisi-kondisi mesin tersebut dapat dikategorikan menjadi 3: A (baik), B (biasa), C (jelek). Berdasarkan hasil inspeksi dari teknisi, manajer produksi membuat keputusan yang terdiri dari:
* *Produksi normal* $(N)$: Teknisi pabrik meninggalkan mesin dengan kondisi yang sekarang, mesin beroperasi secara normal.
* *Perawatan dasar* $(M)$: Teknisi melakukan perawatan dasar di pagi hari dengan biaya 3000 Euro. Perawatan dasar pada mesin dengan keadaan $B$ dapat mengubah kondisi mesin menjadi keadaan $A$ dengan peluang $0.2$ (selain itu tetap pada kondisi B), dan perawatan dasar pada mesin dengan keadaan $C$ dapat mengubah kondisi mesin menjadi keadaan $B$ dengan peluang $0.1$ (selain itu tetap pada kondisi C. Penting untuk dicatat bahwa setelah perawatan dasar di pagi hari, mesin tersebut **tetap beroperasi secara normal**.
* *Overhaul* $(O)$: Perawatan overhaul pada mesin akan selalu mengubah kondisi mesin menjadi keadaan A. Namun, perawatan overhaul memakan waktu sehari penuh dan mesin menjadi tidak produktif pada hari itu. Overhaul memakan biaya 4000 Eur.

Ketika mesin digunakan di dalam produksi, terdapat kemungkinan untuk kondisi mesin memburuk atau bahkan rusak. Peluang kejadian ini dipengaruhi oleh keadaan mesin saat itu. Kerusakan pada mesin dapat berakibat fatal, sehingga mesin membutuhkan 3 hari periode perbaikan (termasuk di hari ketika rusak), selama periode itu mesin tidak dapat digunakan untuk produksi. Pada hari keempat, mesin akan siap digunakan kembali dengan kondisi A. Secara rata-rata, kerusakan pada mesin menyebabkan kerugian total sebesar 23000 Euro, dengan menghitung seluruh biaya tenaga kerja, material, dan kerugian lainnya. Namun, jika mesin tersebut berhasil menyelesaikan satu hari produksi tanpa rusak, maka mesin tersebut dapat memproduksi pasta yang ekivalen dengan keuntungan sebesar 10000 Eur.

Berdasarkan data historis, insinyur kepala dari pabrik pasta tersebut telah menyajikan kita dengan informasi statistik yang berhubungan dengan kemungkinan mesin untuk memburuk dan rusak. Untuk merangkum semua informasi, data yang diberikan oleh insinyur kepala tersebut diberikan di Tabel 1. Namun, beberapa informasi belum diketahui dan kita perlu untuk menghitungnya.

<p align="left">
  <em>Tabel 1. Informasi dari insinyur kepala.</em>
</p>

<p align="center">
  <table>
  <thead>
    <tr>
      <th rowspan="2">Keadaan</th>
      <th rowspan="2">Aksi</th>
      <th colspan="4">Keadaan akhir</th>
    </tr>
    <tr>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>Rusak</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A</td>
      <td>Normal</td>
      <td>0.4</td>
      <td>0.3</td>
      <td>0.2</td>
      <td>0.1</td>
    </tr>
    <tr>
      <td rowspan="3">B</td>
      <td>Normal</td>
      <td>-</td>
      <td>?</td>
      <td>0.4</td>
      <td>0.3</td>
    </tr>
    <tr>
      <td>Maintenance</td>
      <td>0.2</td>
      <td>?</td>
      <td>?</td>
      <td>?</td>
    </tr>
    <tr>
      <td>Overhaul</td>
      <td>1</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td rowspan="3">C</td>
      <td>Normal</td>
      <td>-</td>
      <td>-</td>
      <td>?</td>
      <td>0.4</td>
    </tr>
    <tr>
      <td>Maintenance</td>
      <td>-</td>
      <td>0.1</td>
      <td>?</td>
      <td>?</td>
    </tr>
    <tr>
      <td>Overhaul</td>
      <td>1</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
  </tbody>
  </table>
</p>

Sebagai tambahan, informasi mengenai biaya dan keuntungan adalah sebagai berikut:
* Biaya perawatan dasar ($c_1$) = 3000 Eur
* Biaya overhaul ($c_2$) = 4000 Eur
* Total kerugian kerusakan ($c_3$) = 23000 Eur
* Keuntungan rata-rata harian ($d$) = 10000 Eur

# Perumusan MDP

Formulasi yang akurat dari MDP merupakan hal yang krusial dalam representasi sistem dinamik. Terlebih lagi, dalam kasus ini kita masih memiliki informasi yang perlu untuk dihitung. Artikel ini tidak akan menjelaskan secara mendalam mengenai prosedur untuk mengkonstruksi representasi grafik MDP. Namun, dalam kasus ini akan diberikan hasil akhir dari representasi MDP kemudian akan dijelaskan alasan dibaliknya. Gambar 2 adalah hasil akhir dari representasi MDP untuk permasalahan yang diberikan.

<p align="center">
  <img width="550" src='/images/mdp_tut/mdp_graph.png' class="center">
</p>
<p align="center">
  <em>Gambar 2. Representasi grafik MDP.</em>
</p>

Pertama, dalam kasus ini kita dihadapi dengan permasalahan mesin yang beroperasi 7 hari dalam seminggu dengan ekspektasi penggunaan mesin untuk dalam jangka waktu yang panjang. Sehingga kasus ini dapat disederhanakan sebagai kasis MDP dengan cakrawala tidak terbatas (*infinite horizon*). Dalam kasus ini, kita memiliki keadaan utama dari MDP yaitu $(A,B,C,Br_1)$ yang direpresentasikan oleh bentuk persegi dan aksi utama yaitu operasi normal ($N$), perawatan dasar ($M$), dan overhaul ($O$). Dalam keadaan baik $A$, kita hanya perlu untuk mempertimbangkan aksi operasi normal ($N$) sebagai opsi untuk keadaan ini. Lalu, berdasarkan Tabel 1, kita dapat mengetahui peluang mesin untuk berganti keadaan dan kita dapat menggambarkan panah yang ke keadaan yang berkorespondensi dengan nilai peluang yang ditulis dalam teks berwarna hijau. Hal yang sama juga berlaku untuk keadaan $B$ dan $C$ ketika beroperasi dalam kondisi normal. Namun, dalam tabel satu, informasi $P(B\|B,N)$ dan $P(C\|C,N)$ masih tidak diketahui. Hal ini dapat dengan mudah diselesaikan dengan mengetahui bahwa jumlah dari peluang transisi untuk tiap aksi pada tiap keadaan harus sama dengan 1:

$$
\sum_{s'\in S'} P(s'|s,a) = 1,
$$ 

sehingga kita mendapatkan $P(B\|B,Normal) = 0.3$ dan $P(C\|C,Normal) = 0.6$.

Keadaan menjadi sedikit lebih rumit ketika kita perlu untuk mempertimbangan aksi perawatan dasar. Sebagai contoh, perawatan dasar pada keadaan $B$, kita mengetahui bahwa perawatan dasar tersebut dapat memperbaiki keadaan mesin menjadi $A$ dengan peluang $P(A\|B,M) = 0.2$. Namun, kita juga tahu bahwa mesin tersebut masih dapat beroperasi dengan normal setelah perawatan dasar, sehingga mesin tersebut masih memiliki kemungkinan untuk memburuk atau bahkan rusak. Sehingga peluang transisinya dapat dihitung dengan:
* $P(B\|B,M) = (1 - P(A\|B,M)) \cdot (P(B\|B,N)) = 0.24$
* $P(C\|B,M) = (1 - P(A\|B,M)) \cdot (P(C\|B,N)) = 0.32$
* $P(Br_1\|B,M) = (1 - P(A\|B,M)) \cdot (P(Br_1\|B,N)) = 0.24$.

Dengan prosedur yang serupa, kita dapat menghitung probabilitas transisi untuk perawatan dari keadaan $C$, dengan hasil yang ditampilkan pada Gambar 2. Proses overhaul selalu mengubah kondisi mesin menjadi $A$ dan mesin tersebut tidak dapat beroperasi dengan normal pada hari tersebut, sehingga kita dapat merepresentasikan proses overhaul dengan satu entitas saja. Hal ini dikarenakan tidak ada perbedaan dalam probabiliitas transisi dan nilai imbalan (*reward*) untuk overhaul pada kondisi $B$ maupun $C$.

Memodelkan kerusakan pada mesin dapat sedikit rumit, dinyatakan bahwa:
> Kerusakan pada mesin dapat berakibat fatal, sehingga mesin membutuhkan 3 hari periode perbaikan (termasuk di hari ketika rusak), selama periode itu mesin tidak dapat digunakan untuk produksi. Pada hari keempat, mesin akan siap digunakan kembali dengan kondisi A.

Sehingga, kita dapat mengasumsikan pada hari pertama (hari kerusakan) mesin tersebut masih dalam kondisi sebelumnya dan kita membutuhkan blok ekstra untuk memodelkan 2 hari ekstra dari periode perbaikan. Demi kelengkapan, kita representasikan tiap hari dengan pasangan keadaan dan aksi, hari kedia direpresentasikan dengan pasangan $(Br_1,R)$ dan hari ketiga direpresentasikan oleh $(Br_1,R)$, dimana probabilitas transisinya adalah $P(Br_2\|Br_1,R) = 1$ and $P(A\|Br_2,R) = 1$.

Lalu, kita dapat menghitung nilai ekspektasi imbalan / reward dari tiap aksi pada tiap keadaan dari mesin tersebut. Untuk mensimplifikasi, kita mulai dari yang paling mudah terlebih dahulu, yaitu kerusakan. Kita dapat mengasumsikan bahwa bagian ini memiliki imbalan 0 karena biaya kerugian akibat kerusakan telah dimasukkan kedalam biaya kerugian 23000 Euro, seperti yang dinyatakan dalam rumusan masalah, sehingga:
* $r(R\|Br_1) = 0$
* $r(R\|Br_2) = 0$

Next, the expected reward for performing an overhaul at state $B$ and $C$ is also straightforward. Due to the fact that the machine is unable to operate during that day, the expected reward for performing an overhaul at state $B$ and $C$ yields -4000 Euro reward, the minus sign indicates that the factory is losing money.
* $r(O\|B) = c_2 = -4000$
* $r(O\|C) = c_2 = -4000$

Then, let's consider the normal operation at state $A$, $B$, and $C$. To calculate the expected reward, we must consider that each state has the probability of breakdown, given in Table 1, with total average loss of -23000 Euro, and not breaking down with an expected profit of 10000 Euro. The fact that the machine still generates an expected profit of 10000 regardless of which state the machine will end up in (except breakdown), makes the calculation relatively simple:
* $r(N\|A) = P(Br_1 \| A,N) c_3 + (1-P(Br_1 \| A,N))d = 6700$
* $r(N\|B) = P(Br_1 \| B,N) c_3 + (1-P(Br_1 \| B,N))d = 100$
* $r(N\|C) = P(Br_1 \| C,N) c_3 + (1-P(Br_1 \| C,N))d = -3200$

Finally, we consider the maintenance scenario for states $B$ and $C$. The expected reward computing procedure for these scenarios is exactly the same with the normal operation condition, but with additional basic maintenance cost $c_1$. Hence, the calculation process becomes:
* $r(M\|B) = c_2 + P(Br_1 \| B,M) c_3 + (1-P(Br_1 \| B,M))d = -920$
* $r(M\|C) = c_2 + P(Br_1 \| C,M) c_3 + (1-P(Br_1 \| C,M))d = -4880$

Now, we can put everything together in the MDP where the expected reward is written in red text. From these results, it seems that the maintenance scenario does not offer any benefit since the expected reward costs the factory some money instead of generating profit. But for the details of the optimal policy, we should wait until we get the result from our linear programming method, which will be discussed in the next section.

# Linear programming for solving MDP
Up to this point, you can solve the formulated MDP using other methods such as *value iteration*, *policy iteration*, *dynamic programming*, etc. But, in this tutorial, we will cover solving MDP using linear programming.
>Note: In this tutorial, I'm using [Gurobi](https://www.gurobi.com) because I have an academic license. However, you can also solve the linear programming using open-sourced free library such as [CVXPY](https://www.cvxpy.org) or [Pyomo](http://www.pyomo.org). Their interface might be different from Gurobi, but the general idea should stay the same.

The main objective in solving an MDP (finding the optimal policy) is to maximize the amount of cumulative reward over time. Mathematically, the expression can be written as:

$$
\max \sum_{s\in S}\sum_{a\in A} r(a|s)x_{a|s},
$$

subject to:

$$
\sum_{s\in S}\sum_{a\in A} x_{a|s} = 1,
$$

$$
\sum_{a\in A} x_{a|s} = \sum_{s'\in S}\sum_{a\in A} p(s|s',a)x_{a|s'} \quad \forall s \in S,
$$

$$
x \geq 0
$$

where $x_{a\|s}$ is the probability that we are choosing action $a$ when we are in state $s$ when considering random period. The term $r(a\|s)$ simply states our reward of doing action $a$ in state $s$, and $p(s\|s',a)$ is the transition probability of reaching state $s$ when we are in state $s'$ and taking action $a$. At first, it might seem a little bit confusing. But we will take a closer look when formulating the problem in our LP solver.

First, let's import the Python library and initiate our LP model:

```python
import numpy as np
import gurobipy as gp

lp = gp.Model()
```

Now let's define the decision variables $x_{a\|s}$ to the model. Take note that because we have our third constraint $x \geq 0$, we put it directly in our variable bounds.
```python
# Adding variables to the model
# args `lb=0.0` means the lower bound of the variable is 0.0
x_AN = lp.addVar(name="x_AN", lb=0.0)  
x_BN = lp.addVar(name="x_BN", lb=0.0)
x_BM = lp.addVar(name="x_BM", lb=0.0)
x_BO = lp.addVar(name="x_BO", lb=0.0)
x_CN = lp.addVar(name="x_CN", lb=0.0)
x_CM = lp.addVar(name="x_CM", lb=0.0)
x_CO = lp.addVar(name="x_CO", lb=0.0)
x_Br1R = lp.addVar(name="x_Br1R", lb=0.0)
x_Br2R = lp.addVar(name="x_Br2R", lb=0.0)
```
Considering the objective function of the MDP:

$$
\max \sum_{s\in S}\sum_{a\in A} r(a|s)x_{a|s},
$$

since the $r(a\|s)$ component is already predetermined by the existing information, we can only control the decision variable $x_{a\|s}$. In the objective function context, this roughly translates to "Maximize the total reward by choosing the right decision variable $x_{a\|s}$". Thus for our specific case, the objective function expression can be alternatively written as:

$$
\max r(N|A)x_{N|A} + r(N|B)x_{N|B} + r(M|B)x_{M|B} + r(O|B)x_{O|B} + r(N|C)x_{N|C} + r(M|C)x_{M|C} + r(O|C)x_{O|C} 
$$

For the sake of conciseness, we can directly plug each value of the rewards. In Python, this is written as:

```python
# Define the objective function
lp.setObjective(6700*x_AN + 100*x_BN - 920*x_BM - 4000*x_BO - 3200*x_CN - 4880*x_CM - 4000*x_CO, gp.GRB.MAXIMIZE)
```

Next, we have our first constraint. This constraint simply tells us that the sum of the probability of the action that we take considering a random period should be one. At this point, the decision variable $x_{a\|s}$ might seem a bit abstract. Why do we consider probability to represent a decision that we should take? For now, let's leave it as is and we will come back later in the result discussion.

$$
\sum_{s\in S}\sum_{a\in A} x_{a|s} = 1,
$$

In our case, this expression translates as:

$$
\sum x_{N|A} + x_{N|B} + x_{M|B} + x_{O|B} + x_{N|C} + x_{M|C} + x_{O|C} + x_{R|Br_1} + x_{R|Br_2} = 1
$$

In python:

```python
# Add first constraint
c1 = lp.addConstr(x_AN + x_BN + x_BM + x_BO + x_CN + x_CM + x_CO + x_Br1R + x_Br2R == 1, "action prob")
```

Then, let's take a look at the second constraint. While looks a bit confusing the constraint simply states that for each state node, the "amount" or "magnitude" of the incoming arrow should equal to the outgoing arrow. Thus, this expression:

$$
\sum_{a\in A} x_{a|s} = \sum_{s'\in S}\sum_{a\in A} p(s|s',a)x_{a|s'} \quad \forall s \in S,
$$

Rewritres as:
1. $x_{N\|A} = p(A\|A,N)x_{N\|A} + p(A\|B,M)x_{M\|B} + p(A\|B,O)x_{O\|B} + p(A\|C,O)x_{O\|C} + p(A\|Br_2,R)x_{R\|Br_2}$
2. $x_{N\|B} + x_{M\|B} + x_{O\|B} = p(B\|A,N)x_{N\|A} + p(B\|B,N)x_{N\|B} + p(B\|B,M)x_{M\|B} + p(B\|C,M)x_{M\|C}$
3. $x_{N\|C} + x_{M\|C} + x_{O\|C} = p(C\|A,N)x_{N\|A} + p(C\|B,N)x_{N\|B} + p(C\|B,M)x_{M\|B} + p(C\|C,M)x_{M\|C} + p(C\|C,N)x_{N\|C}$
4. $x_{R\|Br_1} = p(Br_1\|A,N)x_{N\|A} + p(Br_1\|B,N)x_{N\|B} + p(Br_1\|B,M)x_{M\|B} + p(Br_1\|C,M)x_{M\|C} + p(Br_1\|C,N)x_{N\|C}$
5. $x_{R\|Br_2} = p(Br_2\|Br_1,R)x_{R\|Br_1}$

In python, this is written as:

```python
# Add second constraint
c2 = lp.addConstr(x_AN == 0.4*x_AN + 0.2*x_BM + x_BO + x_CO + x_Br2R, "state A constr")
c3 = lp.addConstr(x_BN + x_BM + x_BO == 0.3*x_AN + 0.3*x_BN + 0.24*x_BM + 0.1*x_CM, "state B constr")
c4 = lp.addConstr(x_CN + x_CM + x_CO == 0.2*x_AN + 0.4*x_BN + 0.32*x_BM + 0.6*x_CN + 0.54*x_CM, "state C constr")
c5 = lp.addConstr(x_Br1R == 0.1*x_AN + 0.3*x_BN + 0.24*x_BM + 0.4*x_CN + 0.36*x_CM, "state Br1 constr")
c6 = lp.addConstr(x_Br2R == x_Br1R, "state Br2 constr")
```

Finally, we can run our program:

```python
lp.optimize()
```

While running the program, we should have display as shown in Figure 3 below:

<p align="center">
  <img width="400" src='/images/mdp_tut/running.png' class="center">
</p>
<p align="center">
  <em>Figure 3. Solver display.</em>
</p>

Now, we extract the result:
```python
{var.VarName : var.x for var in lp.getVars()}
``` 

<p align="center">
  <img width="400" src='/images/mdp_tut/result.png' class="center">
</p>
<p align="center">
  <em>Figure 4. Optimal policy.</em>
</p>

Our result shows that the optimal policy is: 

| State  | A      | B        | C        | Br_1   | Br_2   |
|--------|--------|----------|----------|--------|--------|
| Action | Normal | Overhaul | Overhaul | Repair | Repair |

With a maximum reward of $\approx 2765$ Euro.

Now, we get back to interpreting the decision variable. Let's take a look for example $x_{O\|B} \approx 0.1765$, this expression of probability simply translates to the expected frequency of occurrence. Thus, it can be interpreted as "If we follow the optimal policy and run the MDP for 10000 timesteps (days), then we expect to find the machine in state $B$ and make the decision to overhaul the machine approximately 1765 times."

How to cite this article:
```latex
@misc{faza2024mdplptutorial,
   author =       {Faza, Ghifari Adam},
   title =        {Tutorial on solving Markov Decision Process with linear programming},
   month =        {May},
   year =         {2024},
   url =          {https://fazaghifari.github.io/posts/2024/05/mdp-lp-en/},
 }
```
